{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a74b5e-5180-4544-86a0-b047a877eb8e",
   "metadata": {},
   "source": [
    "# Week 5-8 - Coding Quizzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0870791",
   "metadata": {},
   "source": [
    "# Week 5 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc77ea64",
   "metadata": {},
   "source": [
    "In this example, ﻿Z﻿ = the likelihood of a biking accident, ﻿Y﻿ = speed, and ﻿X﻿ = trail difficulty. We assume that ﻿X﻿ decreases ﻿Y﻿ causally because people decrease their speed on difficult trails. In addition, ﻿Y﻿ and ﻿X﻿ both increase ﻿Z﻿ causally because fast biking on difficult trails leads to accidents. Difficulty will be on a scale from 0 to 1, speed in miles per hour, and likelihood of an accident also on a scale from 0 to 1. (Based on the numbers, I'd say these trails are quite challenging!) \n",
    "\n",
    "**Code to create data for this question** \n",
    "num = 100000 \n",
    " \n",
    "difficulty = np.random.uniform(0, 1, (num,)) \n",
    " \n",
    "speed = np.maximum(np.random.normal(15, 5, (num, )) - difficulty * 10, 0) \n",
    " \n",
    "accident = np.minimum(np.maximum(0.03 * speed + 0.4 * difficulty + np.random.normal(0, 0.3, (num,)), 0), 1) \n",
    " \n",
    "df = pd.DataFrame({'difficulty': difficulty, 'speed': speed, 'accident': accident}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee99d9",
   "metadata": {},
   "source": [
    "**1.Use ﻿X﻿ to predict ﻿Y﻿ many times via regression with different data sets. Use many samples in each prediction. Which is closest to the average coefficient of ﻿X﻿ if you do the experiment enough times?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89ae2402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty</th>\n",
       "      <th>speed</th>\n",
       "      <th>accident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.540915</td>\n",
       "      <td>7.978698</td>\n",
       "      <td>0.413691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.468406</td>\n",
       "      <td>12.956855</td>\n",
       "      <td>0.421508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.827738</td>\n",
       "      <td>4.361888</td>\n",
       "      <td>0.360301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.867407</td>\n",
       "      <td>10.090098</td>\n",
       "      <td>0.876056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.768694</td>\n",
       "      <td>3.180727</td>\n",
       "      <td>0.449822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   difficulty      speed  accident\n",
       "0    0.540915   7.978698  0.413691\n",
       "1    0.468406  12.956855  0.421508\n",
       "2    0.827738   4.361888  0.360301\n",
       "3    0.867407  10.090098  0.876056\n",
       "4    0.768694   3.180727  0.449822"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Number of observations\n",
    "num = 100000\n",
    "\n",
    "# X: Trail Difficulty (0 to 1)\n",
    "difficulty = np.random.uniform(0, 1, num)\n",
    "\n",
    "# Y: Speed in mph (decreases as difficulty increases)\n",
    "speed = np.maximum(np.random.normal(15, 5, num) - difficulty * 10, 0)\n",
    "\n",
    "# Z: Likelihood of accident (increases with both speed and difficulty)\n",
    "accident = np.minimum(\n",
    "    np.maximum(0.03 * speed + 0.4 * difficulty + np.random.normal(0, 0.3, num), 0),\n",
    "    1\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'difficulty': difficulty,\n",
    "    'speed': speed,\n",
    "    'accident': accident\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb325152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coefficient of X (difficulty → speed): -9.632\n",
      "Standard deviation of estimates: 0.237\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 1. Generate Full Dataset (as in question)\n",
    "# ----------------------------------------\n",
    "num = 100000\n",
    "\n",
    "difficulty = np.random.uniform(0, 1, num)  # X: trail difficulty\n",
    "speed = np.maximum(np.random.normal(15, 5, num) - difficulty * 10, 0)  # Y: speed\n",
    "accident = np.minimum(\n",
    "    np.maximum(0.03 * speed + 0.4 * difficulty + np.random.normal(0, 0.3, num), 0),\n",
    "    1\n",
    ")  # Z: accident likelihood\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'difficulty': difficulty,\n",
    "    'speed': speed,\n",
    "    'accident': accident\n",
    "})\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Function to generate data (reusable)\n",
    "# ----------------------------------------\n",
    "def generate_data(n=100000):\n",
    "    difficulty = np.random.uniform(0, 1, n)\n",
    "    speed = np.maximum(np.random.normal(15, 5, n) - difficulty * 10, 0)\n",
    "    accident = np.minimum(\n",
    "        np.maximum(0.03 * speed + 0.4 * difficulty + np.random.normal(0, 0.3, n), 0),\n",
    "        1\n",
    "    )\n",
    "    return difficulty, speed, accident\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Function to estimate average regression slope\n",
    "#    (E[Coefficient of difficulty → speed])\n",
    "# -------------------------------------------------\n",
    "def average_regression_coefficient(num_experiments=200, sample_size=5000):\n",
    "    betas = []\n",
    "    for _ in range(num_experiments):\n",
    "        diff, speed, _ = generate_data(n=sample_size)\n",
    "        lr = LinearRegression().fit(diff.reshape(-1, 1), speed)\n",
    "        betas.append(lr.coef_[0])\n",
    "    return np.mean(betas), np.std(betas)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. Run Simulation (Question 1 answer)\n",
    "# ----------------------------------------\n",
    "avg_beta, beta_std = average_regression_coefficient(200, 5000)\n",
    "\n",
    "print(f\"Average coefficient of X (difficulty → speed): {avg_beta:.3f}\")\n",
    "print(f\"Standard deviation of estimates: {beta_std:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8192ce",
   "metadata": {},
   "source": [
    "**2. Then use ﻿X﻿ and ﻿Z﻿ to predict ﻿Y﻿ many times via regression with different datasets. Which of these is closest to the average coefficient of ﻿X﻿? Note: In practice, should we run such a regression? We are controlling for ﻿Z﻿, but ﻿Z﻿ is a collider. That is, ﻿Y﻿ and ﻿X﻿ both cause ﻿Z﻿. Should we control of it or are we better off ignoring ﻿Z﻿? Why or why not?** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5690e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coefficient of X (difficulty → speed | controlling for Z): -10.345\n",
      "Standard deviation: 0.227\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Function to generate data\n",
    "def generate_data(n=100000):\n",
    "    difficulty = np.random.uniform(0, 1, n)\n",
    "    speed = np.maximum(np.random.normal(15, 5, n) - difficulty * 10, 0)\n",
    "    accident = np.minimum(\n",
    "        np.maximum(0.03 * speed + 0.4 * difficulty + np.random.normal(0, 0.3, n), 0),\n",
    "        1\n",
    "    )\n",
    "    return difficulty, speed, accident\n",
    "\n",
    "# Function to estimate average coefficient of X in regression: speed ~ difficulty + accident\n",
    "def average_beta_X_with_collision(num_experiments=200, sample_size=5000):\n",
    "    betas_X = []\n",
    "    \n",
    "    for _ in range(num_experiments):\n",
    "        diff, speed, accident = generate_data(sample_size)\n",
    "\n",
    "        # Stack X and Z (difficulty and accident) as predictors\n",
    "        XZ = np.column_stack([diff, accident])\n",
    "        \n",
    "        # Regress speed on both\n",
    "        lr = LinearRegression().fit(XZ, speed)\n",
    "        \n",
    "        # Coefficient of difficulty (X)\n",
    "        betas_X.append(lr.coef_[0])\n",
    "    \n",
    "    return np.mean(betas_X), np.std(betas_X)\n",
    "\n",
    "# Run the simulation\n",
    "avg_beta_X, std_beta_X = average_beta_X_with_collision(200, 5000)\n",
    "\n",
    "# Display results\n",
    "print(f\"Average coefficient of X (difficulty → speed | controlling for Z): {avg_beta_X:.3f}\")\n",
    "print(f\"Standard deviation: {std_beta_X:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f54bb9",
   "metadata": {},
   "source": [
    "# Week 6 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb704dfb",
   "metadata": {},
   "source": [
    "Assessment Content\n",
    "Given a dataset, match treated (﻿X equals 1﻿) to untreated (﻿X equals 0﻿) based on the confounder (﻿Z﻿).\n",
    "Find the average treatment effect (each item corresponds to one counterfactual) where the counterfactual is the nearest item in the other group (you can use ﻿N e a r e s t N e i g h b o r s﻿ for this.)\n",
    "Find the average treatment effect on the treated, where each treated item corresponds to a counterfactual untreated item, but we otherwise ignore the untreated items.\n",
    "Find the average treatment effect on the untreated, where each untreated item corresponds to a counterfactual treated item, but we otherwise ignore the treated items.\n",
    "Find the optimal treatment effect, which is the maximum treatment effect across all untreated items (i.e., it ends up considering only a single untreated item with its single counterfactual). \n",
    "Use the file homework_6.1.csv. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3609744d",
   "metadata": {},
   "source": [
    "**1.Which is closest to the average treatment effect?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a8c6d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATT (treated only)                    : 1.846\n",
      "ATU (untreated only)                  : 1.549\n",
      "ATE (each unit matched once)          : 1.695\n",
      "Optimal treatment effect among untreated: 2.172\n",
      "Example untreated unit with max effect: {'untreated_index': 152, 'untreated_Y': -1.459379234, 'matched_treated_Y': 0.7130906515770082, 'effect': 2.172469885577008}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 1. Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/joshua-vonkorff/DX702-mod-6/main/homework_6.1.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# 2. Identify key columns\n",
    "y_col = \"Y\"\n",
    "x_col = \"X\"\n",
    "z_cols = [c for c in df.columns if c.startswith(\"Z\")]\n",
    "if not z_cols and \"Z\" in df.columns:\n",
    "    z_cols = [\"Z\"]\n",
    "assert {x_col, y_col}.issubset(df.columns), f\"Dataset missing {x_col} or {y_col}.\"\n",
    "\n",
    "# 3. Split treated / untreated groups \n",
    "treated   = df[df[x_col] == 1].reset_index(drop=True)\n",
    "untreated = df[df[x_col] == 0].reset_index(drop=True)\n",
    "\n",
    "# Feature matrices for matching\n",
    "Z_t = treated[z_cols].to_numpy()\n",
    "Z_u = untreated[z_cols].to_numpy()\n",
    "\n",
    "# 4. Matching nearest neighbor \n",
    "# (A) For treated units → match to untreated\n",
    "nn_u = NearestNeighbors(n_neighbors=1, algorithm=\"auto\").fit(Z_u)\n",
    "dist_tu, idx_tu = nn_u.kneighbors(Z_t, return_distance=True)\n",
    "match_u_for_t = untreated.iloc[idx_tu.ravel()].reset_index(drop=True)\n",
    "\n",
    "# (B) For untreated units → match to treated\n",
    "nn_t = NearestNeighbors(n_neighbors=1, algorithm=\"auto\").fit(Z_t)\n",
    "dist_ut, idx_ut = nn_t.kneighbors(Z_u, return_distance=True)\n",
    "match_t_for_u = treated.iloc[idx_ut.ravel()].reset_index(drop=True)\n",
    "\n",
    "# 5. Compute treatment effect estimates \n",
    "# ATT: treated only\n",
    "att_diffs = treated[y_col].to_numpy() - match_u_for_t[y_col].to_numpy()\n",
    "ATT = att_diffs.mean()\n",
    "\n",
    "# ATU: untreated only\n",
    "atu_diffs = match_t_for_u[y_col].to_numpy() - untreated[y_col].to_numpy()\n",
    "ATU = atu_diffs.mean()\n",
    "\n",
    "# ATE: average across all units (each matched once)\n",
    "ATE = np.concatenate([att_diffs, atu_diffs]).mean()\n",
    "\n",
    "# Optimal treatment effect (max effect among untreated units)\n",
    "optimal_effect_untreated = atu_diffs.max()\n",
    "opt_idx = atu_diffs.argmax()\n",
    "opt_example = {\n",
    "    \"untreated_index\": int(opt_idx),\n",
    "    \"untreated_Y\": float(untreated.iloc[opt_idx][y_col]),\n",
    "    \"matched_treated_Y\": float(match_t_for_u.iloc[opt_idx][y_col]),\n",
    "    \"effect\": float(optimal_effect_untreated),\n",
    "}\n",
    "\n",
    "#  6. Print results \n",
    "print(f\"ATT (treated only)                    : {ATT:.3f}\")\n",
    "print(f\"ATU (untreated only)                  : {ATU:.3f}\")\n",
    "print(f\"ATE (each unit matched once)          : {ATE:.3f}\")\n",
    "print(f\"Optimal treatment effect among untreated: {optimal_effect_untreated:.3f}\")\n",
    "print(\"Example untreated unit with max effect:\", opt_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ca639",
   "metadata": {},
   "source": [
    "**2.Which is closest to the average treatment effect on the treated?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7703dff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect on the Treated (ATT): 1.846\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load data from GitHub\n",
    "url = \"https://raw.githubusercontent.com/joshua-vonkorff/DX702-mod-6/main/homework_6.1.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Columns\n",
    "y_col = \"Y\"   # outcome\n",
    "x_col = \"X\"   # treatment\n",
    "z_cols = [c for c in df.columns if c.startswith(\"Z\")]  # confounder(s)\n",
    "\n",
    "# Split treated and untreated\n",
    "treated = df[df[x_col] == 1].reset_index(drop=True)\n",
    "untreated = df[df[x_col] == 0].reset_index(drop=True)\n",
    "\n",
    "# Extract Z values\n",
    "Z_treated = treated[z_cols].to_numpy()\n",
    "Z_untreated = untreated[z_cols].to_numpy()\n",
    "\n",
    "# Nearest neighbor matching: each treated finds nearest untreated\n",
    "nn = NearestNeighbors(n_neighbors=1).fit(Z_untreated)\n",
    "distances, indices = nn.kneighbors(Z_treated)\n",
    "\n",
    "# Get matched untreated outcomes\n",
    "matched_untreated = untreated.iloc[indices.flatten()][y_col].to_numpy()\n",
    "\n",
    "# ATT = Avg(Y_treated - Y_matched_untreated)\n",
    "ATT = (treated[y_col].to_numpy() - matched_untreated).mean()\n",
    "\n",
    "print(f\"Average Treatment Effect on the Treated (ATT): {ATT:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beeb8bd",
   "metadata": {},
   "source": [
    "**3. Which is closest to the average treatment effect on the untreated?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1efc0e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect on the Untreated (ATU): 1.549\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load data\n",
    "url = \"https://raw.githubusercontent.com/joshua-vonkorff/DX702-mod-6/main/homework_6.1.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Columns\n",
    "y_col = \"Y\"   # outcome\n",
    "x_col = \"X\"   # treatment\n",
    "z_cols = [c for c in df.columns if c.startswith(\"Z\")]  # confounders\n",
    "\n",
    "# Split treated and untreated groups\n",
    "treated = df[df[x_col] == 1].reset_index(drop=True)\n",
    "untreated = df[df[x_col] == 0].reset_index(drop=True)\n",
    "\n",
    "# Extract Z values for matching\n",
    "Z_treated = treated[z_cols].to_numpy()\n",
    "Z_untreated = untreated[z_cols].to_numpy()\n",
    "\n",
    "# Nearest neighbor matching for untreated → find closest treated\n",
    "nn = NearestNeighbors(n_neighbors=1).fit(Z_treated)\n",
    "distances, indices = nn.kneighbors(Z_untreated)\n",
    "\n",
    "# Get matched treated outcomes\n",
    "matched_treated = treated.iloc[indices.flatten()][y_col].to_numpy()\n",
    "\n",
    "# ATU = average of [Y(1) - Y(0) | X=0]\n",
    "atu = (matched_treated - untreated[y_col].to_numpy()).mean()\n",
    "\n",
    "print(f\"Average Treatment Effect on the Untreated (ATU): {atu:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621f1b2",
   "metadata": {},
   "source": [
    "**4.Which is closest to the optimal treatment effect?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "571695f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Treatment Effect: 2.172\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load dataset from GitHub\n",
    "url = \"https://raw.githubusercontent.com/joshua-vonkorff/DX702-mod-6/main/homework_6.1.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Identify columns\n",
    "y_col = \"Y\"\n",
    "x_col = \"X\"\n",
    "z_cols = [c for c in df.columns if c.startswith(\"Z\")]\n",
    "\n",
    "# Split groups\n",
    "treated = df[df[x_col] == 1].reset_index(drop=True)\n",
    "untreated = df[df[x_col] == 0].reset_index(drop=True)\n",
    "\n",
    "# Use Z values for matching\n",
    "Z_treated = treated[z_cols].to_numpy()\n",
    "Z_untreated = untreated[z_cols].to_numpy()\n",
    "\n",
    "# For each untreated, find nearest treated\n",
    "nn = NearestNeighbors(n_neighbors=1).fit(Z_treated)\n",
    "distances, indices = nn.kneighbors(Z_untreated)\n",
    "\n",
    "# Matched treated outcomes\n",
    "matched_treated = treated.iloc[indices.flatten()][y_col].to_numpy()\n",
    "untreated_outcomes = untreated[y_col].to_numpy()\n",
    "\n",
    "# Treatment effect per untreated = Y(1) - Y(0)\n",
    "effects_untreated = matched_treated - untreated_outcomes\n",
    "\n",
    "# Optimal treatment effect (maximum individual effect)\n",
    "optimal_treatment_effect = effects_untreated.max()\n",
    "\n",
    "print(f\"Optimal Treatment Effect: {optimal_treatment_effect:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a167797",
   "metadata": {},
   "source": [
    "# Week 7 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ace19f",
   "metadata": {},
   "source": [
    "Suppose that a process can be modeled via linear regression: \n",
    "\n",
    "W = np.random.normal(0, 1, (1000,))\n",
    "X = W + np.random.normal(0, 1, (1000,)) Z = np.random.normal(0, 1, (1000,)) \n",
    "Y = X + Z + W + np.random.normal(0, 1, (1000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebbd50",
   "metadata": {},
   "source": [
    "**1. Which is closest to the correlation of ﻿X﻿ with the error term in the equation for ﻿Y﻿?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cf500c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between X and error term: 0.018\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "W = np.random.normal(0, 1, 1000)\n",
    "X = W + np.random.normal(0, 1, 1000)\n",
    "Z = np.random.normal(0, 1, 1000)\n",
    "noise = np.random.normal(0, 1, 1000)\n",
    "\n",
    "# True model\n",
    "Y = X + Z + W + noise\n",
    "\n",
    "# Error term (residual from true model)\n",
    "error = Y - (X + Z + W)\n",
    "\n",
    "# Correlation between X and the error\n",
    "corr = np.corrcoef(X, error)[0, 1]\n",
    "print(f\"Correlation between X and error term: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2f44f",
   "metadata": {},
   "source": [
    "**2.If ﻿Y﻿ is written as depending on ﻿X﻿ and ﻿Z﻿ only, ﻿W﻿ is part of the error term. Which, then, is closest to the expected correlation of ﻿X﻿ with the error term in the equation for ﻿Y﻿?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c305747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[ corr(X, error) ] ≈ 0.498  (sd ≈ 0.023)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def corr_X_with_error(n=1000, reps=200, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    cors = []\n",
    "    for _ in range(reps):\n",
    "        W   = rng.normal(0, 1, n)\n",
    "        eta = rng.normal(0, 1, n)   # noise in X\n",
    "        Z   = rng.normal(0, 1, n)\n",
    "        eps = rng.normal(0, 1, n)   # structural noise in Y\n",
    "\n",
    "        X = W + eta\n",
    "        Y = X + Z + W + eps          # true DGP: Y = X + Z + W + eps\n",
    "\n",
    "        # If we (incorrectly) model Y ~ X + Z, the error term is u = W + eps\n",
    "        u = W + eps\n",
    "\n",
    "        cors.append(np.corrcoef(X, u)[0, 1])\n",
    "    return float(np.mean(cors)), float(np.std(cors))\n",
    "\n",
    "avg_corr, sd_corr = corr_X_with_error()\n",
    "print(f\"E[ corr(X, error) ] ≈ {avg_corr:.3f}  (sd ≈ {sd_corr:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa4691",
   "metadata": {},
   "source": [
    "**3.In the data frame for homework_7.1.csv, control for W by regressing ﻿Y﻿ on ﻿X﻿ and ﻿Z﻿ at the following constant values of ﻿W﻿: -1, 0, and 1. (You cannot literally use a constant value of ﻿W﻿, of course, or you will have only one data point! How will you manage this?) The question is: Is the coefficient of ﻿X﻿**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4494677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W ≈ -1 → Coefficient of X = 0.870   (samples = 727)\n",
      "W ≈ 0 → Coefficient of X = 1.381   (samples = 1150)\n",
      "W ≈ 1 → Coefficient of X = 1.963   (samples = 689)\n",
      "\n",
      "✅ The coefficient of X is increasing as W increases.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/joshua-vonkorff/DX702-mod-6/main/homework_7.1.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Function to compute beta_X when W is approximately equal to a chosen value\n",
    "def beta_X_given_W(df, w_value, bandwidth=0.15):\n",
    "    subset = df[np.abs(df['W'] - w_value) < bandwidth]\n",
    "    X_vars = subset[['X', 'Z']]\n",
    "    y = subset['Y']\n",
    "    model = LinearRegression().fit(X_vars, y)\n",
    "    return model.coef_[0], len(subset)   # return coefficient for X and sample size\n",
    "\n",
    "# Evaluate β_X around W = -1, 0, 1\n",
    "results = []\n",
    "for w in [-1, 0, 1]:\n",
    "    beta_x, n = beta_X_given_W(df, w, bandwidth=0.15)\n",
    "    results.append((w, beta_x, n))\n",
    "    print(f\"W ≈ {w} → Coefficient of X = {beta_x:.3f}   (samples = {n})\")\n",
    "\n",
    "# Check if coefficient is increasing\n",
    "if results[0][1] < results[1][1] < results[2][1]:\n",
    "    print(\"\\n✅ The coefficient of X is increasing as W increases.\")\n",
    "else:\n",
    "    print(\"\\n❌ The coefficient of X is NOT consistently increasing with W.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca360cf",
   "metadata": {},
   "source": [
    "**4. def make_error(corr_const, num):** \n",
    "\n",
    " err = list() \n",
    "\n",
    "    prev = np.random.normal(0, 1) \n",
    "\n",
    " for n in range(num): \n",
    "\n",
    "    prev = corr_const * prev + (1 - corr_const) * np.random.normal(0, 1) \n",
    "\n",
    "    err.append(prev) \n",
    "\n",
    "return np.array(err) \n",
    "\n",
    "\n",
    "\n",
    "**Create a linear regression model that uses this function as the error for both (a) the treatment, ﻿X﻿, and (b) the outcome, ﻿Y﻿. (You can use random normal error for any other covariates, if you have them.)** \n",
    "\n",
    "\n",
    "\n",
    "As corr_const increases from 0.2 to 0.5 to 0.8, find (i) the standard deviation of the estimate of the ﻿X﻿ coefficient over many trials, and (ii) the mean of the standard error estimate of the ﻿X﻿ coefficient over many trials. \n",
    "\n",
    "\n",
    "\n",
    "When corr_const increases, then: \n",
    "\n",
    "\n",
    "\n",
    "Hint: don't forget to include an intercept in your regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87202c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_const = 0.2  ->  SD(beta_X) = 0.0315,  mean(SE) = 0.0317,  ratio = 0.996\n",
      "corr_const = 0.5  ->  SD(beta_X) = 0.0410,  mean(SE) = 0.0316,  ratio = 1.294\n",
      "corr_const = 0.8  ->  SD(beta_X) = 0.0667,  mean(SE) = 0.0316,  ratio = 2.111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Given in the prompt\n",
    "# -------------------------------\n",
    "def make_error(corr_const, num):\n",
    "    err = []\n",
    "    prev = np.random.normal(0, 1)\n",
    "    for _ in range(num):\n",
    "        prev = corr_const * prev + (1 - corr_const) * np.random.normal(0, 1)\n",
    "        err.append(prev)\n",
    "    return np.array(err)\n",
    "\n",
    "# -------------------------------\n",
    "# OLS with intercept; return beta_X and its classic OLS SE\n",
    "# -------------------------------\n",
    "def ols_beta_and_se(X, Y):\n",
    "    n = len(Y)\n",
    "    Xmat = np.column_stack([np.ones(n), X])      # intercept + X\n",
    "    # OLS (normal equations)\n",
    "    XtX = Xmat.T @ Xmat\n",
    "    XtX_inv = np.linalg.inv(XtX)\n",
    "    beta_hat = XtX_inv @ (Xmat.T @ Y)\n",
    "    # Residual variance and SE for beta_X (index 1)\n",
    "    resid = Y - Xmat @ beta_hat\n",
    "    dof = n - Xmat.shape[1]\n",
    "    sigma2_hat = (resid @ resid) / dof\n",
    "    var_beta = sigma2_hat * XtX_inv\n",
    "    se_beta_X = np.sqrt(var_beta[1, 1])\n",
    "    return beta_hat[1], se_beta_X   # return beta for X and its SE\n",
    "\n",
    "# -------------------------------\n",
    "# One Monte Carlo block for a given corr_const\n",
    "# -------------------------------\n",
    "def run_block(corr_const, n=1000, reps=500, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    betas = []\n",
    "    ses   = []\n",
    "    for r in range(reps):\n",
    "        # Errors for X and Y have the SAME autocorrelation structure (independent draws)\n",
    "        ex = make_error(corr_const, n)\n",
    "        ey = make_error(corr_const, n)\n",
    "        # Treatment and outcome\n",
    "        X = 1.0 + ex                              # centered-ish regressor with AR(1)-like noise\n",
    "        Y = 2.0 + 1.5 * X + ey                    # true beta_X = 1.5 (intercept included)\n",
    "        bX, seX = ols_beta_and_se(X, Y)\n",
    "        betas.append(bX)\n",
    "        ses.append(seX)\n",
    "    betas = np.array(betas)\n",
    "    ses   = np.array(ses)\n",
    "    sd_beta = betas.std(ddof=1)                   # (i) SD of beta-hat across trials\n",
    "    mean_se = ses.mean()                          # (ii) mean of OLS SE across trials\n",
    "    ratio   = sd_beta / mean_se                   # should increase with corr_const\n",
    "    return sd_beta, mean_se, ratio\n",
    "\n",
    "# -------------------------------\n",
    "# Run for corr_const = 0.2, 0.5, 0.8\n",
    "# -------------------------------\n",
    "for c in [0.2, 0.5, 0.8]:\n",
    "    sd_b, mean_se, ratio = run_block(c, n=1000, reps=600, seed=42)\n",
    "    print(f\"corr_const = {c:.1f}  ->  SD(beta_X) = {sd_b:.4f},  mean(SE) = {mean_se:.4f},  ratio = {ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9d5c7",
   "metadata": {},
   "source": [
    "# Week 8 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff703c1",
   "metadata": {},
   "source": [
    "**Using homework_8.1.csv, find the Average treatment effect with inverse probability weighting. Then, include your code and a written explanation of your work (mentioning any choices or strategies you made in writing the code) in your homework reflection.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752dd515",
   "metadata": {},
   "source": [
    "**Question 1 and 2**\n",
    "1. the ATE is closest to: \n",
    "2. The propensity scores of the first three items in the dataframe are closest to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfe72bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three propensity scores: [0.81, 0.53, 0.66]\n",
      "IPW ATE estimate: 2.271\n",
      "IPW ATE (stabilized): 2.271\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --- load data ---\n",
    "url = \"https://raw.githubusercontent.com/joshua-vonkorff/DX702-mod-6/refs/heads/main/homework_8.1.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Column names used by the assignment\n",
    "T_COL = \"X\"   # treatment (0/1)\n",
    "Y_COL = \"Y\"   # outcome (numeric)\n",
    "# Use every other column as Z (confounders) that predict treatment\n",
    "Z_COLS = [c for c in df.columns if c not in (T_COL, Y_COL)]\n",
    "\n",
    "# One-hot encode any categoricals; leave numbers as-is\n",
    "Z = pd.get_dummies(df[Z_COLS], drop_first=True)\n",
    "\n",
    "# --- 1) Fit propensity model: P(X=1 | Z) ---\n",
    "logit = LogisticRegression(max_iter=2000, solver=\"lbfgs\")\n",
    "logit.fit(Z, df[T_COL])\n",
    "ps = pd.Series(logit.predict_proba(Z)[:, 1], index=df.index, name=\"propensity\")\n",
    "\n",
    "# Show the first three propensity scores (rounded so you can match the MCQ)\n",
    "print(\"First three propensity scores:\", np.round(ps.iloc[:3].to_numpy(), 2).tolist())\n",
    "\n",
    "# --- 2) Compute IPW ATE ---\n",
    "# Weights: 1/p for treated, 1/(1-p) for control (clipped for numerical stability)\n",
    "p = np.clip(ps.values, 1e-6, 1 - 1e-6)\n",
    "treated = df[T_COL].values.astype(int)\n",
    "w = np.where(treated == 1, 1.0 / p, 1.0 / (1.0 - p))\n",
    "\n",
    "# Weighted means by treatment arm\n",
    "y = df[Y_COL].values.astype(float)\n",
    "w1 = w[treated == 1]\n",
    "y1 = y[treated == 1]\n",
    "w0 = w[treated == 0]\n",
    "y0 = y[treated == 0]\n",
    "\n",
    "def wmean(values, weights):\n",
    "    return np.sum(weights * values) / np.sum(weights)\n",
    "\n",
    "mu1 = wmean(y1, w1)\n",
    "mu0 = wmean(y0, w0)\n",
    "ate_ipw = mu1 - mu0\n",
    "print(f\"IPW ATE estimate: {ate_ipw:.3f}\")\n",
    "\n",
    "# stabilized weights (sometimes numerically nicer)\n",
    "p_treat = treated.mean()\n",
    "sw = np.where(treated == 1, p_treat / p, (1 - p_treat) / (1 - p))\n",
    "mu1_sw = wmean(y1, sw[treated == 1])\n",
    "mu0_sw = wmean(y0, sw[treated == 0])\n",
    "ate_ipw_sw = mu1_sw - mu0_sw\n",
    "print(f\"IPW ATE (stabilized): {ate_ipw_sw:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88381d",
   "metadata": {},
   "source": [
    "**Using homework_8.2.csv, match all treated items to the single nearest untreated item using the Mahalanobis distance. (Do this with replacement — the same untreated item can be used again.)** \n",
    "\n",
    "\n",
    "* Use the Mahalanobis function from scipy.spatial.distance \n",
    "\n",
    "* For the inverse covariance matrix, use all ﻿Z 1﻿ values and all ﻿Z 2﻿ values, make them into a ﻿2 x N﻿ matrix, find its ﻿2 x 2﻿ covariance, and invert. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb963d",
   "metadata": {},
   "source": [
    "**Question 3 and 4**\n",
    "\n",
    "3.the ATE is closest to: \n",
    "\n",
    "4. Find the nearest Z1 and Z2 values of the treated item with the least common support (the farthest Mahalanobis distance from the untreated).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3dc68bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mahalanobis NN (with replacement) ATE: 3.438\n",
      "Treated item with least common support (farthest nearest MD):\n",
      "  index: 494\n",
      "  Z1, Z2: (2.7, 0.5)\n",
      "  nearest MD distance: 1.383\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from scipy.spatial.distance import mahalanobis\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "\n",
    "# 1) Load data\n",
    "url = \"https://raw.githubusercontent.com/joshua-vonkorff/DX702-mod-6/refs/heads/main/homework_8.2.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Expect columns: X (treatment 0/1), Y (outcome), Z1, Z2\n",
    "assert {\"X\",\"Y\",\"Z1\",\"Z2\"}.issubset(df.columns), \"Expected columns X, Y, Z1, Z2\"\n",
    "\n",
    "Z_cols = [\"Z1\", \"Z2\"]\n",
    "Z = df[Z_cols].to_numpy()\n",
    "T = df[\"X\"].to_numpy().astype(int)\n",
    "Y = df[\"Y\"].to_numpy().astype(float)\n",
    "\n",
    "# 2) Build inverse covariance of Z (using all rows as instructed)\n",
    "#    cov is 2x2, invert it\n",
    "cov = np.cov(Z.T, bias=False)          # shape (2,2)\n",
    "cov_inv = np.linalg.inv(cov)\n",
    "\n",
    "# Helper to compute Mahalanobis distances between one z and many z0 rows\n",
    "def maha_to_many(z_row, Z_many, VI):\n",
    "    # d^2 = (z - z0)^T VI (z - z0)\n",
    "    diffs = Z_many - z_row\n",
    "    return np.sqrt(np.sum(diffs @ VI * diffs, axis=1))\n",
    "\n",
    "# Split indices\n",
    "idx_treated = np.where(T == 1)[0]\n",
    "idx_control = np.where(T == 0)[0]\n",
    "\n",
    "Z_t = Z[idx_treated]\n",
    "Z_c = Z[idx_control]\n",
    "Y_t = Y[idx_treated]\n",
    "Y_c = Y[idx_control]\n",
    "\n",
    "# 3) For each treated, find nearest untreated by Mahalanobis (with replacement)\n",
    "nearest_control_idx = []\n",
    "nearest_control_dist = []\n",
    "\n",
    "for z in Z_t:\n",
    "    dists = maha_to_many(z, Z_c, cov_inv)\n",
    "    j = np.argmin(dists)\n",
    "    nearest_control_idx.append(idx_control[j])\n",
    "    nearest_control_dist.append(dists[j])\n",
    "\n",
    "nearest_control_idx = np.array(nearest_control_idx)\n",
    "nearest_control_dist = np.array(nearest_control_dist)\n",
    "\n",
    "# 4) Compute ATE = mean(Y_treated - Y_matched_control)\n",
    "Y_matched_c = Y[nearest_control_idx]\n",
    "ate = float(np.mean(Y_t - Y_matched_c))\n",
    "print(f\"Mahalanobis NN (with replacement) ATE: {ate:.3f}\")\n",
    "\n",
    "# 5) Least common support: treated with LARGEST nearest distance\n",
    "worst_treated_pos = int(np.argmax(nearest_control_dist))          # position within treated array\n",
    "worst_treated_idx = int(idx_treated[worst_treated_pos])           # index in original df\n",
    "worst_z1, worst_z2 = Z[worst_treated_idx]\n",
    "\n",
    "print(\"Treated item with least common support (farthest nearest MD):\")\n",
    "print(f\"  index: {worst_treated_idx}\")\n",
    "print(f\"  Z1, Z2: ({worst_z1:.1f}, {worst_z2:.1f})\")\n",
    "print(f\"  nearest MD distance: {nearest_control_dist[worst_treated_pos]:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
